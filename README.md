# cap-benchmark-gcp
This simple script implements the early stages of a GCP-based data warehouse pipeline for the CAP benchmark data.

### Prerequisites
* existing project on [Google Cloud Platform](https://cloud.google.com/).
* APIs enabled
  * [BigQuery API](https://console.cloud.google.com/apis/api/bigquery.googleapis.com)
  * [BigQuery Connection API](https://console.cloud.google.com/apis/api/bigqueryconnection.googleapis.com)
  * [Storage API](https://console.cloud.google.com/apis/api/storage.googleapis.com)  
* private key for a Google Cloud Service Account
  * open [Creditentials](https://console.cloud.google.com/apis/credentials)
  * create/open a service account
  * go to KEYS
  * ADD KEY > Create new key
  * in the dialog window create a JSON key
  * place the JSON file in the same folder as the script
  * rename it to "gcp_key.json"

### Usage
You can pass the required arguments the following way:
```shell
python cap-benchmark-gcp.py folder_path project bucket dataset
```
If you don't pass enough arguments, the script will use parameters in the config file.

### Schema information
Currently table schemas are autogenerated by BigQuery.
For more info check out [cap_pilot_benchmark_xlsx_json](https://github.com/poltextlab/cap_pilot_benchmark_sql_json).

